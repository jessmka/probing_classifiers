{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f942075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import random\n",
    "from probes import *\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "torch.set_printoptions(threshold=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7ab1c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efe91c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('prompt_output.json', 'r') as f:\n",
    "    prompts_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fb66b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.73s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, dtype=torch.float32, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0899de84",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fabb5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "strings = []\n",
    "for demo_dict in prompts_data:\n",
    "    for k, v in demo_dict.items():\n",
    "        question_convos = [\n",
    "                    [{\"role\": \"user\", \"content\": current_convo}]\n",
    "                    for current_convo in v\n",
    "                ]\n",
    "        strings += question_convos\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3204480f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for ques in strings[0:5]:\n",
    "#     print( \n",
    "#         tokenizer.apply_chat_template(\n",
    "#     ques,\n",
    "#     add_generation_prompt=True,\n",
    "#     return_tensors=\"pt\" # Return the input as PyTorch tensors\n",
    "#     )\n",
    "#     )\n",
    "len(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fad1125a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Get model responses - unsteered\n",
    "# ~ 7 mins per question (v slow, see how to speed this up)\n",
    "model_responses = []\n",
    "for ques in strings[0:1]:\n",
    "    tokenized_chat = tokenizer.apply_chat_template(\n",
    "    ques,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\" # Return the input as PyTorch tensors\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            tokenized_chat,\n",
    "            do_sample=False,\n",
    "            max_new_tokens=200,\n",
    "        )\n",
    "    prompt_text = tokenizer.decode(tokenized_chat[0], skip_special_tokens=True)\n",
    "    full_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Get only the new text generated after the prompt\n",
    "    generated_part = full_output[len(prompt_text):].strip()\n",
    "    model_responses.append(generated_part)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ed1fbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edea83c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None]\n",
      "<class 'list'>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Could not infer dtype of NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(decoder_input_ids))\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m20\u001b[39m): \n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     outputs = model(input_ids=input_ids, decoder_input_ids=\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     18\u001b[39m     logits = outputs.logits[:,i,:]\n\u001b[32m     19\u001b[39m     \u001b[38;5;66;03m# perform argmax on the last dimension (i.e. greedy decoding)\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Could not infer dtype of NoneType"
     ]
    }
   ],
   "source": [
    "# Compare with model() instead of model.generate()\n",
    "# https://discuss.huggingface.co/t/generate-without-using-the-generate-method/11379\n",
    "# ~ 7 mins per question (v slow, see how to speed this up)\n",
    "model_responses = []\n",
    "for ques in strings[0:1]:\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "    ques,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\" # Return the input as PyTorch tensors\n",
    "    )#.input_ids()\n",
    "    decoder_input_ids = [model.config.decoder_start_token_id]\n",
    "    \n",
    "    # predicted_ids = []\n",
    "    # print(decoder_input_ids)\n",
    "    # print(type(decoder_input_ids))\n",
    "\n",
    "    # for i in range(20): \n",
    "    #     outputs = model(input_ids=input_ids, decoder_input_ids=torch.tensor([decoder_input_ids]))\n",
    "    #     logits = outputs.logits[:,i,:]\n",
    "    #     # perform argmax on the last dimension (i.e. greedy decoding)\n",
    "    #     predicted_id = logits.argmax(-1)\n",
    "    #     predicted_ids.append(predicted_id.item())\n",
    "    #     print(tokenizer.decode([predicted_id.squeeze()]))\n",
    "    #     # add predicted id to decoder_input_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129aa161",
   "metadata": {},
   "source": [
    "#### Calculate steering vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f732fe3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'probe_pickles/model0.pkl','rb') as f:\n",
    "    x = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b6af187c",
   "metadata": {},
   "outputs": [],
   "source": [
    "probes = {}\n",
    "n_layers = 29\n",
    "for i in range(n_layers):\n",
    "    # probe_pickles/model0.pkl\n",
    "    # print(i)\n",
    "    with open(f'probe_pickles/model{i}.pkl','rb') as f:\n",
    "        x = pickle.load(f)\n",
    "        probes[i] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1c66dbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = probes[28].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f407d372",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5de30f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "def make_steering_hook(w, alpha=1.0):\n",
    "    def hook_fn(module, input, output):\n",
    "        # output shape: [batch, seq_len, hidden_dim]\n",
    "        return output + alpha * w.to(output.device)\n",
    "    return hook_fn\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "layer_to_steer = 20  # pick the layer where your probe was trained\n",
    "alpha = 3.0          # steering strength\n",
    "\n",
    "hook = model.model.layers[layer_to_steer].register_forward_hook(make_steering_hook(w, alpha))\n",
    "prompt = \"The person described in the passage is\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "output = model.generate(**inputs, max_new_tokens=50)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "\n",
    "hook.remove()  # clean up after steering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d9799063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Jin, I'd be happy to help you find some classes at the University of Queensland that might interest you.\\n\\nBased on your preferences, here are some class suggestions:\\n\\n**Engineering-related classes:**\\n\\n1. Engineering Management: This class will help you understand the business side of engineering and how to apply engineering principles to real-world problems.\\n2. Operations Research: This class will teach you how to use mathematical and analytical techniques to optimize business processes and solve complex problems.\\n3. Entrepreneurship: This class will help you develop the skills and knowledge needed to start and run your own business.\\n\\n**Economics-related classes:**\\n\\n1. Microeconomics: This class will introduce you to the fundamental concepts of microeconomics, including supply and demand, market structures, and consumer behavior.\\n2. Macroeconomics: This class will cover the broader topics of macroeconomics, including economic growth, inflation, and international trade.\\n3. Econometrics: This class will teach you how to use statistical techniques to\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77d7586",
   "metadata": {},
   "source": [
    "### Use probes created in probes.ipynb to steer the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3563b39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/Veranep/implicit-personalization-stereotypes/blob/main/mitigate.py#L48\n",
    "def optimize_one_inter_rep(\n",
    "    inter_rep, layer_name, target, probe, mult, normalized=False\n",
    "):\n",
    "    \"\"\"Add probe weights to model's internal representations\"\"\"\n",
    "    global first_time\n",
    "    tensor = (inter_rep.clone()).to(\"cuda\").requires_grad_(True)\n",
    "    rep_f = lambda: tensor\n",
    "    probe_weights = torch.from_numpy(probe.coef_[target]).to(\"cuda\")\n",
    "\n",
    "    if normalized:\n",
    "        cur_input_tensor = (\n",
    "            rep_f() + probe_weights * mult * 100 / rep_f().norm()\n",
    "        )\n",
    "    else:\n",
    "        cur_input_tensor = rep_f() + probe_weights * mult\n",
    "    return cur_input_tensor.clone()\n",
    "\n",
    "def edit_inter_rep_multi_layers(output, layer_name):\n",
    "    \"\"\"Apply probe weight adding to all relevant layers of the model\"\"\"\n",
    "    layer_num = int(\n",
    "        layer_name[layer_name.rfind(\"model.layers.\") + len(\"model.layers.\") :]\n",
    "    )\n",
    "    probe = probes_dict[layer_num]\n",
    "    cloned_inter_rep = (\n",
    "        output[0][:, -1].unsqueeze(0).detach().clone().to(torch.float)\n",
    "    )\n",
    "    with torch.enable_grad():\n",
    "        cloned_inter_rep = optimize_one_inter_rep(\n",
    "            cloned_inter_rep,\n",
    "            layer_name,\n",
    "            cf_target,\n",
    "            probe,\n",
    "            mult=mult,\n",
    "            normalized=False,\n",
    "        )\n",
    "    output[0][:, -1] = cloned_inter_rep.to(torch.float16)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4070e956",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_list = []\n",
    "for i in range(29):\n",
    "    # load\n",
    "    with open(f'probe_pickles/model{i}.pkl', 'rb') as f:\n",
    "        mod_list.append(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02896d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/Veranep/implicit-personalization-stereotypes/blob/main/eval_conversations.py#L299\n",
    "\n",
    "def modified_model(\n",
    "    model,\n",
    "    probes,\n",
    "    modified_layer_names,\n",
    "    demographic,\n",
    "    value,\n",
    "    batch_size,\n",
    "    question_convos,\n",
    "    N,\n",
    "):\n",
    "    \"\"\"Apply steering towards demographic group to model and generate response\"\"\"\n",
    "    global probes_dict\n",
    "    probes_dict = probes\n",
    "    global mult\n",
    "    mult = N\n",
    "    global cf_target\n",
    "    cf_target = probe_targets[demographic][value]\n",
    "    with TraceDict(\n",
    "        model.model,\n",
    "        modified_layer_names,\n",
    "        edit_output=edit_inter_rep_multi_layers,\n",
    "    ) as ret:\n",
    "        model_answer = [\n",
    "            answer[0][\"generated_text\"][-1][\"content\"]\n",
    "            for answer in tqdm(\n",
    "                model(\n",
    "                    question_convos,\n",
    "                    batch_size=batch_size,\n",
    "                    do_sample=False,\n",
    "                    max_new_tokens=100,\n",
    "                ),\n",
    "                total=len(question_convos),\n",
    "            )\n",
    "        ]\n",
    "    return model_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98104f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/Veranep/implicit-personalization-stereotypes/blob/main/eval_conversations.py#L299\n",
    "new_answers = modified_model(\n",
    "                model,\n",
    "                probes,\n",
    "                modified_layer_names,\n",
    "                demographic,\n",
    "                target,\n",
    "                batch_size,\n",
    "                question_convos,\n",
    "                N,\n",
    "            )\n",
    "return_convos[i][\"mod_indirect_question\"][k] = new_answers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probing_classifiers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
