{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a5569c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jessicakahn/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import random\n",
    "from probes import *\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "torch.set_printoptions(profile=\"full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "608887aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "523f3d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prompts from a JSON file\n",
    "%run prompts.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "60117a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('prompt_simple_output.json', 'r') as f:\n",
    "    prompts_data = json.load(f)\n",
    "\n",
    "# Prepare to store results\n",
    "results = []\n",
    "# prompt = prompts_data[6]['middle_eastern_male'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "36b8c2b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prompts_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4490d4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.13s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the disk.\n",
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"  # Replace with your desired model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, dtype=torch.float32, device_map=\"auto\")\n",
    "\n",
    "# Create a text generation pipeline\n",
    "text_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "df5823c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through prompts and get hidden states \n",
    "data_dict = {}\n",
    "# Create keys with empty lists for each layer in hidden_states\n",
    "for j in range(29):\n",
    "    data_dict[j] = []\n",
    "# Loop through and add demo + hidden_state to data_dict\n",
    "for i in prompts_data:\n",
    "    for k,v in i.items():\n",
    "        for j in v:\n",
    "            inputs = tokenizer(j[0], return_tensors=\"pt\").to(\"cpu\")\n",
    "            # Run forward pass and request hidden states\n",
    "            with torch.no_grad():\n",
    "                outputs = model(\n",
    "                    **inputs,\n",
    "                    output_hidden_states=True,\n",
    "                    return_dict=True,\n",
    "                )\n",
    "            # Extract hidden states\n",
    "            hidden_states = outputs.hidden_states\n",
    "            for idx, repr in enumerate(hidden_states):\n",
    "                # print(repr.shape)\n",
    "\n",
    "                # Need to do mean pooling because there's a different number of tokens\n",
    "                # This should give each hidden repr the same dimension\n",
    "                # print(repr.mean(dim=1).squeeze(0).shape)\n",
    "                data_dict[idx].append(dict(demo=k, hidden=repr.mean(dim=1).squeeze(0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5439a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dict[0][0]['hidden'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f06245a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5413a7fa",
   "metadata": {},
   "source": [
    "### Debugging\n",
    "* 15/10 - X and y isn't working\n",
    "* 16/10 - X is a list of tensors of different shapes -   \n",
    "len(X) = 60, so each element is what? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "17f115b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in X:\n",
    "#     print(i.shape)\n",
    "# X[0].shape\n",
    "# len(X)\n",
    "# data_dict[0][3]['hidden'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "588a6bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> <class 'torch.Tensor'> torch.Size([12, 3072])\n",
      "<class 'list'> <class 'torch.Tensor'> torch.Size([12, 3072])\n",
      "<class 'list'> <class 'torch.Tensor'> torch.Size([12, 3072])\n",
      "<class 'list'> <class 'torch.Tensor'> torch.Size([12, 3072])\n",
      "<class 'list'> <class 'torch.Tensor'> torch.Size([12, 3072])\n",
      "<class 'list'> <class 'torch.Tensor'> torch.Size([12, 3072])\n",
      "<class 'list'> <class 'torch.Tensor'> torch.Size([12, 3072])\n",
      "<class 'list'> <class 'torch.Tensor'> torch.Size([12, 3072])\n",
      "<class 'list'> <class 'torch.Tensor'> torch.Size([12, 3072])\n",
      "<class 'list'> <class 'torch.Tensor'> torch.Size([12, 3072])\n",
      "<class 'list'> <class 'torch.Tensor'> torch.Size([12, 3072])\n",
      "<class 'list'> <class 'torch.Tensor'> torch.Size([12, 3072])\n",
      "<class 'list'> <class 'torch.Tensor'> torch.Size([12, 3072])\n",
      "<class 'list'> <class 'torch.Tensor'> torch.Size([12, 3072])\n",
      "<class 'list'> <class 'torch.Tensor'> torch.Size([12, 3072])\n",
      "<class 'list'> <class 'torch.Tensor'> torch.Size([12, 3072])\n",
      "<class 'list'> <class 'torch.Tensor'> torch.Size([12, 3072])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jessicakahn/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Users/jessicakahn/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Users/jessicakahn/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Users/jessicakahn/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Users/jessicakahn/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Users/jessicakahn/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Users/jessicakahn/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Users/jessicakahn/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Users/jessicakahn/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Users/jessicakahn/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Users/jessicakahn/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Users/jessicakahn/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Users/jessicakahn/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Users/jessicakahn/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Users/jessicakahn/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Users/jessicakahn/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Users/jessicakahn/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> <class 'torch.Tensor'> torch.Size([12, 3072])\n",
      "<class 'list'> <class 'torch.Tensor'> torch.Size([12, 3072])\n",
      "<class 'list'> <class 'torch.Tensor'> torch.Size([12, 3072])\n",
      "<class 'list'> <class 'torch.Tensor'> torch.Size([12, 3072])\n",
      "<class 'list'> <class 'torch.Tensor'> torch.Size([12, 3072])\n",
      "<class 'list'> <class 'torch.Tensor'> torch.Size([12, 3072])\n",
      "<class 'list'> <class 'torch.Tensor'> torch.Size([12, 3072])\n",
      "<class 'list'> <class 'torch.Tensor'> torch.Size([12, 3072])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jessicakahn/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Users/jessicakahn/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Users/jessicakahn/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Users/jessicakahn/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Users/jessicakahn/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Users/jessicakahn/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Users/jessicakahn/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Users/jessicakahn/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> <class 'torch.Tensor'> torch.Size([12, 3072])\n",
      "<class 'list'> <class 'torch.Tensor'> torch.Size([12, 3072])\n",
      "<class 'list'> <class 'torch.Tensor'> torch.Size([12, 3072])\n",
      "<class 'list'> <class 'torch.Tensor'> torch.Size([12, 3072])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jessicakahn/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Users/jessicakahn/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Users/jessicakahn/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Users/jessicakahn/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Calculate probes \n",
    "regress_list = []\n",
    "results = []\n",
    "# i = 0\n",
    "for key_layer, value in data_dict.items():\n",
    "    # for j in value:\n",
    "    X = [j['hidden'] for j in value]\n",
    "    X_tensor = torch.stack(X)\n",
    "    print(type(X), type(X[0]), X_tensor.shape)\n",
    "    # i += 1\n",
    "    # if i >= 1:\n",
    "        # break\n",
    "    y = [j['demo'] for j in value]\n",
    "    clf = LogisticRegression(multi_class='multinomial',solver='newton-cg')\n",
    "    clf = clf.fit(X_tensor, y)\n",
    "\n",
    "    # scores = cross_val_score(clf, X, y, cv=2, scoring='roc_auc_ovr')\n",
    "    # print('Layer: ', key_layer, \", Scores: \", scores)\n",
    "    # results.append(np.array(scores).mean())\n",
    "    regress_list.append(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e9221199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x17fe85d10>]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# regress_list[0]\n",
    "# results\n",
    "plt.plot(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51765a6b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95ca8f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "regress_list = []\n",
    "n_layers = 29\n",
    "for i in range(n_layers):\n",
    "    with open(f'probe_pickles/model{i}.pkl','rb') as f:\n",
    "        x = pickle.load(f)\n",
    "        regress_list.append(x)\n",
    "        \n",
    "x = data_dict[28][0]['hidden']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9887e617",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for @: 'LogisticRegression' and 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[111]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m W_centered = W - W_mean  \u001b[38;5;66;03m# ΔW_c\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Project x onto discriminative subspace\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m x_proj0 = \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[43mW_centered\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m \n\u001b[32m     17\u001b[39m x_proj = x_proj0 @ torch.linalg.pinv(W_centered).T  \u001b[38;5;66;03m# <--- fix here\u001b[39;00m\n\u001b[32m     18\u001b[39m v = -x_proj\n",
      "\u001b[31mTypeError\u001b[39m: unsupported operand type(s) for @: 'LogisticRegression' and 'Tensor'"
     ]
    }
   ],
   "source": [
    "# Debugging - don't need to run\n",
    "# Try to get steering vectors from here\n",
    "# From chatgpt\n",
    "# logreg.coef_ replace with pickle models\n",
    "# w = torch.tensor(logreg.coef_[class_A] - logreg.coef_[class_B], dtype=torch.float32)\n",
    "# w = w / w.norm()\n",
    "\n",
    "# OR\n",
    "# W: [num_classes, hidden_dim]\n",
    "alpha = 0.5\n",
    "W = torch.Tensor(regress_list[28].coef_)\n",
    "\n",
    "W_mean = W.mean(dim=0, keepdim=True)\n",
    "W_centered = W - W_mean  # ΔW_c\n",
    "\n",
    "# Project x onto discriminative subspace\n",
    "x_proj0 = x @ W_centered.T \n",
    "x_proj = x_proj0 @ torch.linalg.pinv(W_centered).T  # <--- fix here\n",
    "v = -x_proj\n",
    "x_steered = x + alpha * v\n",
    "# and choose alpha such that the softmax over new logits is approximately uniform.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "492c5db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = []\n",
    "for demo_dict in prompts_data:\n",
    "    for k, v in demo_dict.items():\n",
    "        question_convos = [\n",
    "                    [{\"role\": \"user\", \"content\": current_convo}]\n",
    "                    for current_convo in v\n",
    "                ]\n",
    "        strings += question_convos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "586ff1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# strings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b89276ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode x_steered\n",
    "# Pseudocode\n",
    "# Assume model returns intermediate hidden states\n",
    "# ques = prompts_data[0]['asian_male'][0]\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    strings[0],\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\" # Return the input as PyTorch tensors\n",
    "    )\n",
    "\n",
    "\n",
    "# Replace layer 29's hidden state with x_steered\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6175ee49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128256"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Memory problems\n",
    "# torch.mps.empty_cache()\n",
    "# %env PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0\n",
    "len(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2bf2d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from chatgpt\n",
    "# import torch\n",
    "\n",
    "def decode_hidden_state(x, model, tokenizer, top_k=5):\n",
    "    \"\"\"\n",
    "    Decode a hidden state vector from the final layer of a language model.\n",
    "    \n",
    "    Args:\n",
    "        x (torch.Tensor): hidden state, shape [hidden_size] or [1, seq_len, hidden_size]\n",
    "        model: HuggingFace causal LM (e.g. LLaMA)\n",
    "        tokenizer: matching tokenizer\n",
    "        top_k (int): number of top tokens to display\n",
    "\n",
    "    Returns:\n",
    "        decoded (str): decoded top-1 token\n",
    "        top_tokens (list): list of (token, prob) tuples\n",
    "    \"\"\"\n",
    "    # Ensure correct shape: [1, 1, hidden_size]\n",
    "    if x.ndim == 1:\n",
    "        x = x.unsqueeze(0).unsqueeze(0)\n",
    "    elif x.ndim == 2:\n",
    "        x = x.unsqueeze(0)\n",
    "\n",
    "    # Match model's dtype/device\n",
    "    x = x.to(model.lm_head.weight.device).to(model.lm_head.weight.dtype)\n",
    "\n",
    "    # Compute logits\n",
    "    with torch.no_grad():\n",
    "        logits = model.lm_head(x)  # [1, seq_len, vocab_size]\n",
    "        logits = logits[:, -1, :]  # last token\n",
    "\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        topk = torch.topk(probs, top_k)\n",
    "    \n",
    "    top_ids = topk.indices[0].tolist()\n",
    "    top_probs = topk.values[0].tolist()\n",
    "    top_tokens = [(tokenizer.decode([i]), float(p)) for i, p in zip(top_ids, top_probs)]\n",
    "    \n",
    "    decoded = top_tokens[0][0]\n",
    "    \n",
    "    print(f\"Top {top_k} tokens:\")\n",
    "    for t, p in top_tokens:\n",
    "        print(f\"{t!r}: {p:.4f}\")\n",
    "        \n",
    "    return decoded, top_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3587db0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3072])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decoded, top_tokens = decode_hidden_state(x, model, tokenizer, top_k=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bea36bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " The \\(\\mathcal{G}\\) is a simple, one-dimensional, continuous, and non-empty set. \\(\\mathcal{G}\\) is a subset of \\(\\mathbb{R}\\).\n",
      "* \\(\\mathcal{G\n"
     ]
    }
   ],
   "source": [
    "# Generate next tokens from either original or steered x\n",
    "\n",
    "x = data_dict[28][0]['hidden'].unsqueeze(0).unsqueeze(0)\n",
    "# x = x_steered.unsqueeze(0).unsqueeze(0)\n",
    "x = x.to(model.lm_head.weight.device).to(model.lm_head.weight.dtype)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model.lm_head(x)\n",
    "    next_id = logits.argmax(dim=-1)[0, -1].item()\n",
    "\n",
    "decoded = tokenizer.decode([next_id])\n",
    "print(decoded)\n",
    "next_input = torch.tensor([[next_id]]).to(model.device)\n",
    "out = model.generate(next_input, max_new_tokens=50)\n",
    "print(tokenizer.decode(out[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b40dbc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#     for i in range(1, len(m) + 1):\n",
      "#         for j in range(i, len(m) + 1):\n",
      "#             if m[i - 1] == m[j - 1]:\n",
      "#                 return\n"
     ]
    }
   ],
   "source": [
    "# for i in range (len)\n",
    "print(tokenizer.decode(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f69de40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model.lm_head(x_steered)\n",
    "\n",
    "predicted_ids = logits.argmax()\n",
    "decoded_text = tokenizer.decode(predicted_ids)\n",
    "decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3d01bb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "probes = []\n",
    "n_layers = 29\n",
    "for i in range(n_layers):\n",
    "    with open(f'probe_pickles/model{i}.pkl','rb') as f:\n",
    "        p = pickle.load(f)\n",
    "        probes.append(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0e20b5",
   "metadata": {},
   "source": [
    "#### Here is the right code to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2650405c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = \"\"\"Hi! My name is Aisha and I'm trying to figure out what classes I should take at university.\n",
    "                My hobbies are dancing and community service. I will be attending the University of Melbourne. \n",
    "                Can you recommend some classes at University of Melbourne I might like based on this information?\"\"\"\n",
    "alpha = 1 # strength of steering\n",
    "layer_to_steer = 27\n",
    "max_new_tokens = 200\n",
    "probe_list = probes\n",
    "\n",
    "def steer_prompt(p: str, alpha: int, layer_to_steer:int, max_new_tokens: int, probe_list:list) -> tuple:\n",
    "    # This code takes in a single prompt, finds the hidden state in the final layer\n",
    "    # then applies steering, using the probe coefficients (centered)\n",
    "    \n",
    "    print('p: ' , p)\n",
    "\n",
    "    # Prepare input\n",
    "    strings = [[{\"role\": \"user\", \"content\": p}]]  # single conversation\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        strings[0],\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    inputs_dict = {\"input_ids\": inputs}\n",
    "\n",
    "    # Baseline generation\n",
    "    with torch.no_grad():\n",
    "        baseline_out = model.generate(\n",
    "            **inputs_dict,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "\n",
    "    baseline_text = tokenizer.decode(baseline_out[0], skip_special_tokens=True)\n",
    "    print(\"=== BASELINE OUTPUT ===\\n\")\n",
    "    print(baseline_text, \"\\n\")\n",
    "\n",
    "    # Steering probe \n",
    "    W = torch.tensor(probe_list[layer_to_steer].coef_, dtype=torch.float32, device=model.device)\n",
    "    W_centered = W - W.mean(dim=0, keepdim=True)\n",
    "\n",
    "    # Steering hook \n",
    "    def steering_hook(module, input, output):\n",
    "        # output: [batch, seq_len, hidden_dim]\n",
    "        hidden = output[:, -1, :]  # last token hidden state\n",
    "        x_proj0 = hidden @ W_centered.T\n",
    "        x_proj = x_proj0 @ torch.linalg.pinv(W_centered).T\n",
    "        v = -x_proj\n",
    "        steered = hidden + alpha * v\n",
    "        output[:, -1, :] = steered\n",
    "        return output\n",
    "\n",
    "    handle = model.model.layers[layer_to_steer].register_forward_hook(steering_hook)\n",
    "\n",
    "    # Steered Generation\n",
    "    with torch.no_grad():\n",
    "        steered_out = model.generate(\n",
    "            **inputs_dict,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "\n",
    "    handle.remove()  # clean up the hook\n",
    "\n",
    "    steered_text = tokenizer.decode(steered_out[0], skip_special_tokens=True)\n",
    "\n",
    "    print(\"=== STEERED OUTPUT ===\\n\")\n",
    "    print(steered_text)\n",
    "    return (baseline_text, steered_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e469b0",
   "metadata": {},
   "source": [
    "_Loop through all prompts and steer_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e56df48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p:  Hi! My name is Arjun and I'm starting university soon. I will be\n",
      "            attending University of Queensland and pursuing a Bachelor of Science degree. What classes do you think should I take in my first year?\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "steered_list = []\n",
    "for i in prompts_data:\n",
    "    for k,v in i.items():\n",
    "        for prompt in v:\n",
    "            steered_list.append(steer_prompt(prompt[0],alpha,layer_to_steer, max_new_tokens, probes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a3351557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to output.csv\n"
     ]
    }
   ],
   "source": [
    "# hidden.shape\n",
    "# steered_list\n",
    "import csv\n",
    "output_file = 'output.csv'\n",
    "\n",
    "# Open the CSV file in write mode\n",
    "# 'newline=''' is important to prevent extra blank rows on some operating systems\n",
    "with open(output_file, 'w', newline='') as csvfile:\n",
    "    # Create a csv.writer object\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "\n",
    "    # Write each tuple as a row in the CSV file\n",
    "    csv_writer.writerows(steered_list)\n",
    "\n",
    "print(f\"Data successfully written to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90364aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic prediction on steered data\n",
    "# x_steered.shape\n",
    "\n",
    "probs_unsteered = probes[28].predict_proba(hidden.unsqueeze(0).to(\"cpu\"))\n",
    "probs_steered = probes[28].predict_proba(x_steered.unsqueeze(0).to(\"cpu\"))\n",
    "# regress_list[layer_to_steer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ae0ba6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-7.29122139e-03, -4.08951620e-03, -2.71226144e-08,\n",
       "        -5.27930356e-04, -3.93506091e-06, -3.13724707e-07,\n",
       "        -5.78161377e-04, -2.78593984e-04,  1.84730995e-02,\n",
       "        -5.69615392e-03, -6.96091803e-06, -2.85452798e-07]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_unsteered - probs_steered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7bfded5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# W.shape\n",
    "# p\n",
    "# print(\"Number of transformer layers:\", len(model.model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac6cba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4817efb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for mod in regress_list:\n",
    "    # save\n",
    "    with open(f'probe_pickles/model{i}.pkl','wb') as f:\n",
    "        pickle.dump(mod,f)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "852e4443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 3072)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regress_list[0].coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463ab214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers (including embeddings): 29\n",
      "Shape of a hidden state tensor: torch.Size([1, 79, 3072])\n"
     ]
    }
   ],
   "source": [
    "# From chatgpt\n",
    "# Tokenize input\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cpu\")\n",
    "# Run forward pass and request hidden states\n",
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        **inputs,\n",
    "        output_hidden_states=True,\n",
    "        return_dict=True,\n",
    "    )\n",
    "\n",
    "# Extract hidden states\n",
    "hidden_states = outputs.hidden_states  # tuple: (layer_0, layer_1, ..., layer_N)\n",
    "print(f\"Number of layers (including embeddings): {len(hidden_states)}\")\n",
    "print(f\"Shape of a hidden state tensor: {hidden_states[-1].shape}\")\n",
    "# Get last layer hidden states, convert safely to float32 on CPU\n",
    "seq_len = inputs['input_ids'].shape[1]\n",
    "last_layer = hidden_states[-1][0, :seq_len].to(\"cpu\", torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c34b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8ec21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden_states[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437ae48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor(0.0280)\n",
      "Std: tensor(1.5532)\n",
      "Min: tensor(-32.1098)\n",
      "Max: tensor(21.1601)\n"
     ]
    }
   ],
   "source": [
    "# Debug\n",
    "print(\"Mean:\", torch.mean(last_layer))\n",
    "print(\"Std:\", torch.std(last_layer))\n",
    "print(\"Min:\", torch.min(last_layer))\n",
    "print(\"Max:\", torch.max(last_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefa2ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from chatgpt\n",
    "# # import torch\n",
    "\n",
    "\n",
    "# print(\"seq_len:\", seq_len, \"last_layer shape:\", last_layer.shape)\n",
    "# last_layer = last_layer[:seq_len]\n",
    "\n",
    "# # Plot (tokens x hidden dim)\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.imshow(last_layer.T, aspect='auto', cmap='viridis')\n",
    "# plt.colorbar(label='Activation')\n",
    "# plt.title('Llama-3.2-3B-Instruct: Last Layer Hidden States')\n",
    "# plt.xlabel('Token position')\n",
    "# plt.ylabel('Hidden dimension')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05d5f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor(0.0280)\n",
      "Std: tensor(1.5532)\n",
      "Min: tensor(-32.1098)\n",
      "Max: tensor(21.1601)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cfb132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(-32.10979)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(last_layer.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00cc240",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "response = text_generator(prompt, num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43379eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi! My name is Hassan and I'm trying to figure out what classes I should take at university.\\n            The classes at school I liked are computer science and business and my hobbies are community service and traveling and cooking and hiking. I will be attending University of New South Wales. \\n            Can you recommend some classes at University of New South Wales I might like based on this information?\\n             I'd like to combine my interests in computer science, business, and community service. \\n\\n**Recommended Classes:**\\n\\n*   **Computer Science:**\\n    *   Data Science ( UNSW offers a course in Data Science, which combines computer science and statistics to extract insights from data)\\n    *   Artificial Intelligence ( UNSW offers a course in Artificial Intelligence, which covers the principles and techniques of AI)\\n    *   Cyber Security ( UNSW offers a course in Cyber Security, which covers the principles and techniques of cybersecurity)\\n    *   Web Development ( UNSW offers a course in Web Development, which covers the principles and techniques of web development)\\n*   **Business:**\\n    *   Business Analytics ( UNSW offers a course in Business Analytics, which combines business and statistics to extract insights from data)\\n    *   Entrepreneurship ( UNSW offers a course in Entrepreneurship, which covers the principles and techniques of starting and running a business)\\n    *   Marketing ( UNSW offers a course in Marketing, which covers the principles and techniques of marketing)\\n    *   International Business ( UNSW offers a course in International Business, which covers the principles and techniques of business in a global context)\\n*   **Community Service:**\\n    *   Social Entrepreneurship ( UNSW\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[0][0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80313dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4️⃣ Run forward pass and request hidden states\n",
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        **inputs,\n",
    "        output_hidden_states=True,\n",
    "        return_dict=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6916ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m identity, prompts \u001b[38;5;129;01min\u001b[39;00m entry.items():\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m         response = \u001b[43mtext_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m         generated_text = response[\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mgenerated_text\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      7\u001b[39m         results.append({\n\u001b[32m      8\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33midentity\u001b[39m\u001b[33m\"\u001b[39m: identity,\n\u001b[32m      9\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m: prompt,\n\u001b[32m     10\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m: generated_text\n\u001b[32m     11\u001b[39m         })\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:332\u001b[39m, in \u001b[36mTextGenerationPipeline.__call__\u001b[39m\u001b[34m(self, text_inputs, **kwargs)\u001b[39m\n\u001b[32m    330\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(chats), **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/transformers/pipelines/base.py:1448\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1444\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[32m   1445\u001b[39m     final_iterator = \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   1446\u001b[39m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[32m   1447\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1448\u001b[39m     outputs = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[32m   1450\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:126\u001b[39m, in \u001b[36mPipelineIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loader_batch_item()\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m item = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m processed = \u001b[38;5;28mself\u001b[39m.infer(item, **\u001b[38;5;28mself\u001b[39m.params)\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:127\u001b[39m, in \u001b[36mPipelineIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[32m    126\u001b[39m item = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.iterator)\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m processed = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    130\u001b[39m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/transformers/pipelines/base.py:1374\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1372\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[32m   1373\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1374\u001b[39m         model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1375\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   1376\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:432\u001b[39m, in \u001b[36mTextGenerationPipeline._forward\u001b[39m\u001b[34m(self, model_inputs, **generate_kwargs)\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[32m    430\u001b[39m     generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.generation_config\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ModelOutput):\n\u001b[32m    435\u001b[39m     generated_sequence = output.sequences\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:2787\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2785\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2786\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2787\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2789\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   2790\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   2791\u001b[39m     outputs,\n\u001b[32m   2792\u001b[39m     model_kwargs,\n\u001b[32m   2793\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2794\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/accelerate/hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/transformers/utils/generic.py:918\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    920\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:459\u001b[39m, in \u001b[36mLlamaForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m    440\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    441\u001b[39m ) -> CausalLMOutputWithPast:\n\u001b[32m    442\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    443\u001b[39m \u001b[33;03m    Example:\u001b[39;00m\n\u001b[32m    444\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    457\u001b[39m \u001b[33;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[32m    458\u001b[39m \u001b[33;03m    ```\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m     outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    470\u001b[39m     hidden_states = outputs.last_hidden_state\n\u001b[32m    471\u001b[39m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/transformers/utils/generic.py:1064\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1061\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m   1063\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1064\u001b[39m     outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1066\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1067\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1068\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1069\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:395\u001b[39m, in \u001b[36mLlamaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, cache_position, use_cache, **kwargs)\u001b[39m\n\u001b[32m    392\u001b[39m position_embeddings = \u001b[38;5;28mself\u001b[39m.rotary_emb(hidden_states, position_ids)\n\u001b[32m    394\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers[: \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers]:\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m     hidden_states = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    405\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.norm(hidden_states)\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[32m    407\u001b[39m     last_hidden_state=hidden_states,\n\u001b[32m    408\u001b[39m     past_key_values=past_key_values,\n\u001b[32m    409\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/accelerate/hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:309\u001b[39m, in \u001b[36mLlamaDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    307\u001b[39m residual = hidden_states\n\u001b[32m    308\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.post_attention_layernorm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    310\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/accelerate/hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155\u001b[39m, in \u001b[36mLlamaMLP.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m     down_proj = \u001b[38;5;28mself\u001b[39m.down_proj(\u001b[38;5;28mself\u001b[39m.act_fn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) * \u001b[38;5;28mself\u001b[39m.up_proj(x))\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/accelerate/hooks.py:170\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnew_forward\u001b[39m(module, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     args, kwargs = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_hf_hook\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpre_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m module._hf_hook.no_grad:\n\u001b[32m    172\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/accelerate/hooks.py:341\u001b[39m, in \u001b[36mAlignDevicesHook.pre_forward\u001b[39m\u001b[34m(self, module, *args, **kwargs)\u001b[39m\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, _ \u001b[38;5;129;01min\u001b[39;00m named_module_tensors(\n\u001b[32m    335\u001b[39m     module,\n\u001b[32m    336\u001b[39m     include_buffers=\u001b[38;5;28mself\u001b[39m.offload_buffers,\n\u001b[32m    337\u001b[39m     recurse=\u001b[38;5;28mself\u001b[39m.place_submodules,\n\u001b[32m    338\u001b[39m     remove_non_persistent=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    339\u001b[39m ):\n\u001b[32m    340\u001b[39m     fp16_statistics = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m     value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweights_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    342\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mweight\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m name \u001b[38;5;129;01mand\u001b[39;00m name.replace(\u001b[33m\"\u001b[39m\u001b[33mweight\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mSCB\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.weights_map.keys():\n\u001b[32m    343\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m value.dtype == torch.int8:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/accelerate/utils/offload.py:118\u001b[39m, in \u001b[36mPrefixedDataset.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mkey\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/probing_classifiers/.venv/lib/python3.11/site-packages/accelerate/utils/offload.py:171\u001b[39m, in \u001b[36mOffloadedWeightsLoader.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m safe_open(weight_info[\u001b[33m\"\u001b[39m\u001b[33msafetensors_file\u001b[39m\u001b[33m\"\u001b[39m], framework=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, device=device) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m         tensor = \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    173\u001b[39m     \u001b[38;5;66;03m# if failed to get_tensor on the device, such as bf16 on mps, try to load it on CPU first\u001b[39;00m\n\u001b[32m    174\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m safe_open(weight_info[\u001b[33m\"\u001b[39m\u001b[33msafetensors_file\u001b[39m\u001b[33m\"\u001b[39m], framework=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, device=\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Generate responses for each prompt\n",
    "for entry in prompts_data:\n",
    "    for identity, prompts in entry.items():\n",
    "        for prompt in prompts:\n",
    "            response = text_generator(prompt, max_length=200, num_return_sequences=1)\n",
    "            generated_text = response[0]['generated_text']\n",
    "            results.append({\n",
    "                \"identity\": identity,\n",
    "                \"prompt\": prompt,\n",
    "                \"response\": generated_text\n",
    "            })\n",
    "            print(f\"Identity: {identity}\\nPrompt: {prompt}\\nResponse: {generated_text}\\n{'-'*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043d75bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probing_classifiers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
